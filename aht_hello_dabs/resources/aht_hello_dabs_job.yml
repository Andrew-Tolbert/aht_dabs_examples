resources:
  jobs:
    aht_hello_dabs_job:
      name: aht_hello_dabs_job

      # Run this job once an hour.
      trigger:
        periodic:
          interval: 1
          unit: HOURS

      tasks:
        - task_key: task_0
          spark_python_task:
            python_file: ../src/task.py
         # The key that references an environment spec in a job.
          environment_key: default

        - task_key: task_1
          notebook_task:
            notebook_path: ../src/notebook.ipynb

      # A list of task execution environment specifications that can be referenced by tasks of this job.
      environments:
        - environment_key: default

          # Full documentation of this spec can be found at:
          # https://docs.databricks.com/api/workspace/jobs/create#environments-spec
          spec:
            client: "1"
            dependencies:
              - cowsay
      parameters:
        - name: job_env
          default: ${var.var_env} 



# # The main job for aht_hello_dabs.
# resources:
#   jobs:
#     aht_hello_dabs_job:
#       name: aht_hello_dabs_job

#       schedule:
#         # Run every day at 8:37 AM
#         quartz_cron_expression: '44 37 8 * * ?'
#         timezone_id: Europe/Amsterdam

#       email_notifications:
#         on_failure:
#           - andrew.tolbert@databricks.com

#       tasks:
#         - task_key: notebook_task
#           job_cluster_key: job_cluster
#           notebook_task:
#             notebook_path: ../src/notebook.ipynb
        
#         - task_key: main_task
#           depends_on:
#             - task_key: notebook_task
          
#           job_cluster_key: job_cluster
#           python_wheel_task:
#             package_name: aht_hello_dabs
#             entry_point: main
#           libraries:
#             # By default we just include the .whl file generated for the aht_hello_dabs package.
#             # See https://docs.databricks.com/dev-tools/bundles/library-dependencies.html
#             # for more information on how to add other libraries.
#             - whl: ../dist/*.whl

#       job_clusters:
#         - job_cluster_key: job_cluster
#           new_cluster:
#             spark_version: 13.3.x-scala2.12
#             node_type_id: i3.xlarge
#             autoscale:
#                 min_workers: 1
#                 max_workers: 4
